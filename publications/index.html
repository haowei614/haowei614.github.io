<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Haowei Cheng | Publications</title>
  <meta name="description" content="This is the personal website of Haowei Cheng">
  <link rel="canonical" href="/publications/">

  <link rel="preconnect" href="https://fonts.googleapis.com"> 
  <link href="https://fonts.googleapis.com/css2?family=DM+Serif+Display:ital@0;1&family=DM+Sans:wght@300;400;500&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/assets/css/site-v2.css">

</head>
<body>
  <canvas id="bg-canvas"></canvas>
  <div class="cursor-glow" id="cursorGlow"></div>

  <nav>
    <a href="/" class="nav-logo">Haowei Cheng</a>
    <ul class="nav-links">
      <li><a href="/">About</a></li>
<li><a href="/publications/" class="active">Publications</a></li>
<li><a href="/projects/">Projects</a></li>
<li><a href="/news/">News</a></li>
    </ul>
  </nav>

  <main>
    <div class="page-header">
      <p class="section-label">// publications</p>
      <h1 class="section-title">Research Papers & PDF Links</h1>
    </div>

    <div class="content">
    
  <h1>Publications</h1>
  <h6><nobr><em>*</em></nobr> denotes equal contribution and joint lead authorship.</h6>

<!-- ==============2025============= -->

<section class="year-group" aria-labelledby="year-2025">
<div class="year-divider">
  <div>
    <h3 class="bibliography-year" id="year-2025">2025</h3>
  </div>
  <div>
    <!-- News items for 2025 -->
    <ol class="bibliography"><li><article class="paper-item">
      <div class="abbr">
        <a class="badge light-green darken-1" style="width: 65px;" href="https://ieee-jp.org/section/tokyo/event/r10htc2025/" target="_blank">
          IEEE R10 HTC2025
        </a>
      </div>
      <div>
    
        <div id="Analysis_of_STM">
          <h5 class="title">Translating Requirements into CARLA Executable Scripts: an LLM-Driven Automated Scenario Realization</h5>
          <div class="author">
                    <nobr>Shiyang Guan,</nobr>
                  
                      <nobr>Yijun Lu,</nobr>
      
                      <nobr>Jati H. Husen,</nobr>
      
                      <nobr><em>Haowei Cheng</em>,</nobr>
      
                      <nobr><a href="http://www.washi.cs.waseda.ac.jp/washizaki/" target="_blank">Hironori Washizaki</a>,</nobr>
      
                      <nobr><a href="https://researchmap.jp/ubayashi" target="_blank">Naoyasu Ubayashi</a>,</nobr>
                  and  
                      <nobr><a href="https://researchmap.jp/nobukazu" target="_blank">Nobukazu Yoshioka</a></nobr>
          </div>
      
          <div>
            <p class="periodical font-italic">
              
            </p>
          </div>
        
          <div>
            
              <a class="badge grey" data-toggle="collapse" href="#HTC-abstract" role="button" aria-expanded="false" aria-controls="HTC-abstract">Abstract</a>
              <a class="badge grey" href="/assets/pdf/D2/IEEE_HTC.pdf" target="_blank">PDF</a>
              <a class="badge grey" href="" target="_blank">Poster</a>
            </div>

          <div>
            <div id="HTC-abstract" class="collapse">
              <div class="abstract">
                Scenario-based testing is critical for ensuring the safety and robustness of autonomous driving (AD) systems, 
                particularly in extreme scenarios such as heavy rain, pedestrian-involved crashes, and nighttime conditions. 
                Our previous work integrated the CARLA simulator with the Multi-view Modeling Framework for ML Systems (M3S), 
                facilitating scenario generation but still requiring substantial manual scripting. In this paper, we extend our
                 framework by incorporating Large Language Models (LLMs) with M3S to automate scenario generation. Using a hierarchical 
                 prompt design, our approach extracts structured parameters from M3S descriptions into a JSON schema, which guides the LLM 
                 to generate accurate CARLA simulation scripts. Our evaluation demonstrates significant improvements in automation accuracy 
                 and efficiency, substantially reducing manual intervention and enhancing continuous testing cycles for AD systems.
              </div>
            </div>
          </div>
          
        </div>
      </div>
      </article></li></ol>


    <ol class="bibliography"><li><article class="paper-item">
      <div class="abbr">
        <a class="badge light-green darken-1" style="width: 65px;" href="https://www.ipsj.or.jp/kenkyukai/event/se219.html" target="_blank">
          IPSJ
        </a>
      </div>
      <div>
    
        <div id="Analysis_of_STM">
          <h5 class="title">Enhancing Safety in Autonomous Driving: Integrating CARLA for Multi-Sensor Dataset Generation and Advanced Scenario Testing</h5>
          <div class="author">
                    <nobr>Shiyang Guan,</nobr>
                  
                      <nobr>Yijun Lu,</nobr>
      
                      <nobr>Jati H. Husen,</nobr>
      
                      <nobr><em>Haowei Cheng</em>,</nobr>
      
                      <nobr><a href="http://www.washi.cs.waseda.ac.jp/washizaki/" target="_blank">Hironori Washizaki</a>,</nobr>
      
                      <nobr><a href="https://researchmap.jp/ubayashi" target="_blank">Naoyasu Ubayashi</a>,</nobr>
                  and  
                      <nobr><a href="https://researchmap.jp/nobukazu" target="_blank">Nobukazu Yoshioka</a></nobr>
          </div>
      
          <div>
            <p class="periodical font-italic">
              
            </p>
          </div>
        
          <div>
            
              <a class="badge grey" data-toggle="collapse" href="#IPSJ-abstract" role="button" aria-expanded="false" aria-controls="SLR-abstract">Abstract</a>
              <a class="badge grey" href="/assets/pdf/D1/IPSJ.pdf" target="_blank">PDF</a>
              <a class="badge grey" href="" target="_blank">Poster</a>
            
         
            </div>
        
          
          <div>
            <div id="IPSJ-abstract" class="collapse">
              <div class="abstract">
                Simulation environments are vital to autonomous driving research, enabling safe and 
                cost- effective studies of dense traffic, adverse weather, and sidewalk navigation. 
                Yet real-world data collection for these scenarios can be hazardous and expensive. 
                To address this, we integrate the CARLA simulator for dataset generation and scenario construction. 
                Our approach leverages CARLA’s autopilot to capture traffic-sign data via LiDAR detection and 
                semantic segmentation, and employs a custom manual script for sidewalk data. We also vary weather 
                and traffic density, using RoadRunner for specialized maps. Preliminary results suggest CARLA-generated 
                data helps identify domain gaps when combined with real GTSRB data, and improves segmentation (IoU) 
                in sidewalk scenes. Looking ahead, we propose automated scenario generation integrating with M3S. 
                Engineers can define high-level objectives and then incorporate them into CARLA, ensuring robust 
                evaluations for critical autonomous driving scenarios.
              </div>
            </div>
          </div>
          
        </div>
      </div>
      </article></li></ol>

    <!-- ------ -->
    <ol class="bibliography"><li><article class="paper-item">
    <div class="abbr">
      <a class="badge light-green darken-1" style="width: 65px;" href="https://conf.researchr.org/home/chase-2025" target="_blank">
        CHASE
      </a>
    </div>
    <div>
  
      <div id="Analysis_of_STM">
        <h5 class="title">VRTopic: Advancing Topic Modeling in Virtual Reality User Reviews with Large Language Models</h5>
        <div class="author">
                  <nobr>Yijun Lu,</nobr>
                
                    <nobr><em>Haowei Cheng</em>,</nobr>
    
                    <nobr>Jati H. Husen,</nobr>
    
                    <nobr><a href="http://www.washi.cs.waseda.ac.jp/washizaki/" target="_blank">Hironori Washizaki</a>,</nobr>
    
                    <nobr><a href="https://researchmap.jp/ubayashi" target="_blank">Naoyasu Ubayashi</a>,</nobr>
                and  
                    <nobr><a href="https://researchmap.jp/nobukazu" target="_blank">Nobukazu Yoshioka</a></nobr>
        </div>
    
        <div>
          <p class="periodical font-italic">
            
          </p>
        </div>
      
        <div>
          
            <a class="badge grey" data-toggle="collapse" href="#VRTopic-abstract" role="button" aria-expanded="false" aria-controls="SLR-abstract">Abstract</a>
            <a class="badge grey" href="/assets/pdf/D1/CHASE_DECS.pdf" target="_blank">PDF</a>
            <a class="badge grey" href="" target="_blank">Poster</a>
          
       
          </div>
      
        
        <div>
          <div id="VRTopic-abstract" class="collapse">
            <div class="abstract">
              With the rapid development of Virtual Reality (VR) technology, effectively understanding user 
              feedback has become a core task for improving user experience and optimizing system functionality. 
              However, extracting meaningful insights from VR user reviews remains challenging. Traditional topic 
              modeling methods often generate unannotated and ambiguous topics, requiring extensive manual annotation 
              and analysis. To address this issue, this study proposes an innovative approach that leverages 
              state-of-the-art Large Language Models (LLMs) to automatically identify and precisely summarize key topics
              from VR user reviews. Ultimately, this research aims to generate accurate topics from VR-related textual 
              inputs that genuinely reflect user concerns. By filling the gap in the application of LLMs to VR text 
              analysis, this study provides VR developers with precise user insights, aiding product optimization and 
              iterative improvement.
            </div>
          </div>
        </div>
        
      </div>
    </div>
    </article></li></ol>
  </div>
</div>




</section>

<!-- ==============2024============= -->

<section class="year-group" aria-labelledby="year-2024">
<div class="year-divider">
  <div>
    <h3 class="bibliography-year" id="year-2024">2024</h3>
  </div>
  <div>
    <!-- News items for 2024 -->
    <ol class="bibliography"><li><article class="paper-item">
    <div class="abbr">
      <a class="badge light-green darken-1" style="width: 65px;" href="https://arxiv.org/abs/2409.06741" target="_blank">
        arXiv
      </a>
    </div>
    <div>
  
      <div id="Analysis_of_STM">
        <h5 class="title">Generative AI for Requirements Engineering: A Systematic Literature Review.</h5>
        <div class="author">
                  <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>,</nobr>
                
                    <nobr>Jati H. Husen,</nobr>
    
                    <nobr>Sien Reeve Peralta,</nobr>
    
                    <nobr>Bowen Jiang,</nobr>
    
                    <nobr><a href="https://researchmap.jp/nobukazu" target="_blank">Nobukazu Yoshioka</a>,</nobr>
    
                    <nobr><a href="https://researchmap.jp/ubayashi" target="_blank">Naoyasu Ubayashi</a>,</nobr>
                and  
                    <nobr><a href="http://www.washi.cs.waseda.ac.jp/washizaki/" target="_blank">Hironori Washizaki</a></nobr>
        </div>
    
        <div>
          <p class="periodical font-italic">
            
          </p>
        </div>
      
        <div>
          
            <a class="badge grey" data-toggle="collapse" href="#SLR-abstract" role="button" aria-expanded="false" aria-controls="SLR-abstract">Abstract</a>
            <a class="badge grey" href="/assets/pdf/D1/SLR.pdf" target="_blank">PDF</a>
            <a class="badge grey" href="/assets/pdf/D1/SLR_Poster.pdf" target="_blank">Poster</a>
          
       
          </div>
      
        
        <div>
          <div id="SLR-abstract" class="collapse">
            <div class="abstract">
              Context: Generative AI (GenAI) has emerged as a transformative tool in software engineering, with requirements engineering (RE) actively exploring its potential to revolutionize processes and outcomes. The integration of GenAI into RE presents both promising opportunities and significant challenges that necessitate systematic analysis and evaluation.

              Objective: This paper presents a comprehensive systematic literature review (SLR) analyzing state-of-the-art applications and innovative proposals leveraging GenAI in RE. It surveys studies focusing on the utilization of GenAI to enhance RE processes while identifying key challenges and opportunities in this rapidly evolving field. 

              Method: A rigorous SLR methodology was used to conduct an in-depth analysis of 27 carefully selected primary studies. The review examined research questions pertaining to the application of GenAI across various RE phases, the models and techniques used, and the challenges encountered in implementation and adoption.

              Results: The most salient findings include i: a predominant focus on the early stages of RE, particularly the elicitation and analysis of requirements, indicating potential for expansion into later phases; ii: the dominance of large language models, especially the GPT series, highlighting the need for diverse AI approaches; and iii: persistent challenges in domain-specific applications and the interpretability of AI-generated outputs, underscoring areas requiring further research and development.

              Conclusions: The results highlight the critical need for comprehensive evaluation frameworks, improved human–AI collaboration models, and thorough consideration of ethical implications in GenAI-assisted RE. Future research should prioritize extending GenAI applications across the entire RE lifecycle, enhancing domain-specific capabilities, and developing strategies for responsible AI integration in RE practices.
        
            </div>
          </div>
        </div>
        
      </div>
    </div>
    </article></li></ol>
  </div>
</div>
</section>



<!-- ==============2023============= -->


<section class="year-group" aria-labelledby="year-2023">
<div class="year-divider">
  <div>
    <h3 class="bibliography-year" id="year-2023">2023</h3>
  </div>
  <div>
    <ol class="bibliography">
      <li>
        <div>
          <div class="abbr">
            <a class="badge light-green darken-1" style="width: 65px;" href="http://www.apsipa.org/asc_web/apsipa2023/" target="_blank">
              APSIPA
            </a>
          </div>
          
          <div>
            <div id="Analysis_of_STM">
              <h5 class="title">Analysis of Spectro-Temporal Modulation Representation for Deep-Fake Speech Detection.</h5>
              <div class="author">
                <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>,</nobr>
                <nobr>Candy Olivia Mawalim,</nobr>
                <nobr>Kai Li,</nobr>
                <nobr>Lijun Wang,</nobr>
                and  
                <nobr><a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=293&l=1" target="_blank">Masashi Unoki</a>,</nobr>
              </div>

              <div>
                <p class="periodical font-italic">
                  The 15th Asia-Pasific Signal and Information Processing Association (APSIPA ASC 2023), Taipei, Taiwan, 31 October - 3 November 2023.
                </p>
              </div>
            
              <div>
                <a class="badge grey" data-toggle="collapse" href="#deepfake-abstract" role="button" aria-expanded="false" aria-controls="deepfake-abstract">Abstract</a>
                <a class="badge grey" href="/assets/pdf/deepfakes/Analysis_of_Spectro-Temporal_Modulation_Representation_for_Deep-Fake_Speech_Detection.pdf" target="_blank">PDF</a>
                <a class="badge grey" href="/assets/pdf/deepfakes/deepfake_detection.pdf" target="_blank">Poster</a>
              </div>
            
              <div>
                <div id="deepfake-abstract" class="collapse">
                  <div class="abstract">
                    Deep-fake speech detection aims to develop effective techniques for identifying fake speech generated using advanced deep-learning methods. 
                    It can reduce the negative impact of malicious production or dissemination of fake speech in real-life scenarios. Although humans can relatively easy to distinguish between genuine and fake speech due to 
                    human auditory mechanisms, it is difficult for machines to distinguish them correctly. One major reason for this challenge is that machines struggle to effectively separate speech content from human vocal system information. 
                    Common features used in speech processing face difficulties in handling this issue, hindering the neural network from learning the discriminative differences between genuine and fake speech. To address this issue, 
                    we investigated spectro-temporal modulation representations in genuine and fake speech, which simulate the human auditory perception process. Next, the spectro-temporal modulation was fit to a light convolutional neural network 
                    bidirectional long short-term memory for classification. We conducted experiments on the benchmark datasets of the Automatic Speaker Verification and Spoofing Countermeasures Challenge 2019 (ASVspoof2019) and the Audio Deep synthesis 
                    Detection Challenge 2023 (ADD2023), achieving an equal-error rate of 8.33\% and 42.10\%, respectively. The results showed that spectro-temporal modulation representations could distinguish the genuine and deep-fake speech and have adequate 
                    performance in both datasets.
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>

      <li>
        <div>
          <div class="abbr">
            <a class="badge light-green darken-1" style="width: 65px;" href="https://www.ieice.org/hokuriku/" target="_blank">
              JHES
            </a>
          </div>
          
          <div>
            <div id="deep-fake">
              <h5 class="title">Study on Deep-Fake Speech Detection Based on Spectro-Temporal Modulation Representation.</h5>
              <div class="author">
                <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>, Candy Olivia Mawalim, Kai Li, and Masashi Unoki</nobr>
              </div>

              <div>
                <p class="periodical font-italic">
                  In Proc. Joint conference of Hokuriku chapters of Electrical and information Societies (JHES 2023).
                </p>
              </div>
            
              <div>
                <a class="badge grey" href="/assets/pdf/deepfakes/jhes2023_Haowei_Final.pdf" target="_blank">PDF</a>
                <a class="badge grey" href="/assets/pdf/deepfakes/JHES_Haowei_Slide.pdf" target="_blank">Slide</a>
              </div>
            
              <div>
                <div id="agile_modeling-abstract" class="collapse">
                  <div class="abstract">
                    [摘要内容保持不变]
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>
    </ol>
  </div>
</div>
</section>
  </main>

  <footer>
    &copy; 2026 Haowei Cheng · Built on GitHub Pages
  </footer>
  <script src="/assets/js/site-v2.js"></script>

</body>
</html>
