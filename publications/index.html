<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Haowei Cheng | Publications</title>
  <meta name="description" content="This is the personal website of Haowei Cheng">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Haowei</span> Cheng</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About     
            </a>
          </li>

              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    Curriculum Vitae      
                  </a>
              </li>

              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                  </a>
              </li>

              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                      <span class="sr-only">(current)</span>
                  </a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/news/">
                  News
                </a>
              </li>
 
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Publications</h1>
  <h6><nobr><em>*</em></nobr> denotes equal contribution and joint lead authorship.</h6>
<p></p>



<!-- ==============2025============= -->

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
  <div class="col-sm-1 mt-2 p-0 pr-1">
    <h3 class="bibliography-year">2025</h3>
  </div>
  <div class="col-sm-11 p-0">
    <!-- News items for 2025 -->
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://ieee-jp.org/section/tokyo/event/r10htc2025/" target="_blank">
          IEEE R10 HTC2025
        </a>
      </div>
      <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
        <div id="Analysis_of_STM" class="col p-0">
          <h5 class="title mb-0">Translating Requirements into CARLA Executable Scripts: an LLM-Driven Automated Scenario Realization</h5>
          <div class="author">
                    <nobr>Shiyang Guan,</nobr>
                  
                      <nobr>Yijun Lu,</nobr>
      
                      <nobr>Jati H. Husen,</nobr>
      
                      <nobr><em>Haowei Cheng</em>,</nobr>
      
                      <nobr><a href="http://www.washi.cs.waseda.ac.jp/washizaki/" target="_blank">Hironori Washizaki</a>,</nobr>
      
                      <nobr><a href="https://researchmap.jp/ubayashi" target="_blank">Naoyasu Ubayashi</a>,</nobr>
                  and  
                      <nobr><a href="https://researchmap.jp/nobukazu" target="_blank">Nobukazu Yoshioka</a></nobr>
          </div>
      
          <div>
            <p class="periodical font-italic">
              
            </p>
          </div>
        
          <div class="col p-0">
            
              <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#HTC-abstract" role="button" aria-expanded="false" aria-controls="SLR-abstract">Abstract</a>
              <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/D2/IEEE_HTC.pdf" target="_blank">PDF</a>
              <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Poster</a>
            </div>
               
          <div class="col mt-2 p-0">
            <div id="HTC-abstract" class="collapse">
              <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                Scenario-based testing is critical for ensuring the safety and robustness of autonomous driving (AD) systems, 
                particularly in extreme scenarios such as heavy rain, pedestrian-involved crashes, and nighttime conditions. 
                Our previous work integrated the CARLA simulator with the Multi-view Modeling Framework for ML Systems (M3S), 
                facilitating scenario generation but still requiring substantial manual scripting. In this paper, we extend our
                 framework by incorporating Large Language Models (LLMs) with M3S to automate scenario generation. Using a hierarchical 
                 prompt design, our approach extracts structured parameters from M3S descriptions into a JSON schema, which guides the LLM 
                 to generate accurate CARLA simulation scripts. Our evaluation demonstrates significant improvements in automation accuracy 
                 and efficiency, substantially reducing manual intervention and enhancing continuous testing cycles for AD systems.
              </div>
            </div>
          </div>
          
        </div>
      </div>
      </div></li></ol>


    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
      <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://www.ipsj.or.jp/kenkyukai/event/se219.html" target="_blank">
          IPSJ
        </a>
      </div>
      <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
        <div id="Analysis_of_STM" class="col p-0">
          <h5 class="title mb-0">Enhancing Safety in Autonomous Driving: Integrating CARLA for Multi-Sensor Dataset Generation and Advanced Scenario Testing</h5>
          <div class="author">
                    <nobr>Shiyang Guan,</nobr>
                  
                      <nobr>Yijun Lu,</nobr>
      
                      <nobr>Jati H. Husen,</nobr>
      
                      <nobr><em>Haowei Cheng</em>,</nobr>
      
                      <nobr><a href="http://www.washi.cs.waseda.ac.jp/washizaki/" target="_blank">Hironori Washizaki</a>,</nobr>
      
                      <nobr><a href="https://researchmap.jp/ubayashi" target="_blank">Naoyasu Ubayashi</a>,</nobr>
                  and  
                      <nobr><a href="https://researchmap.jp/nobukazu" target="_blank">Nobukazu Yoshioka</a></nobr>
          </div>
      
          <div>
            <p class="periodical font-italic">
              
            </p>
          </div>
        
          <div class="col p-0">
            
              <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#IPSJ-abstract" role="button" aria-expanded="false" aria-controls="SLR-abstract">Abstract</a>
              <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/D1/IPSJ.pdf" target="_blank">PDF</a>
              <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Poster</a>
            
         
            </div>
        
          
          <div class="col mt-2 p-0">
            <div id="IPSJ-abstract" class="collapse">
              <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                Simulation environments are vital to autonomous driving research, enabling safe and 
                cost- effective studies of dense traffic, adverse weather, and sidewalk navigation. 
                Yet real-world data collection for these scenarios can be hazardous and expensive. 
                To address this, we integrate the CARLA simulator for dataset generation and scenario construction. 
                Our approach leverages CARLA’s autopilot to capture traffic-sign data via LiDAR detection and 
                semantic segmentation, and employs a custom manual script for sidewalk data. We also vary weather 
                and traffic density, using RoadRunner for specialized maps. Preliminary results suggest CARLA-generated 
                data helps identify domain gaps when combined with real GTSRB data, and improves segmentation (IoU) 
                in sidewalk scenes. Looking ahead, we propose automated scenario generation integrating with M3S. 
                Engineers can define high-level objectives and then incorporate them into CARLA, ensuring robust 
                evaluations for critical autonomous driving scenarios.
              </div>
            </div>
          </div>
          
        </div>
      </div>
      </div></li></ol>

    <!-- ------ -->
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-1 p-0 abbr">
      <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://conf.researchr.org/home/chase-2025" target="_blank">
        CHASE
      </a>
    </div>
    <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
  
      <div id="Analysis_of_STM" class="col p-0">
        <h5 class="title mb-0">VRTopic: Advancing Topic Modeling in Virtual Reality User Reviews with Large Language Models</h5>
        <div class="author">
                  <nobr>Yijun Lu,</nobr>
                
                    <nobr><em>Haowei Cheng</em>,</nobr>
    
                    <nobr>Jati H. Husen,</nobr>
    
                    <nobr><a href="http://www.washi.cs.waseda.ac.jp/washizaki/" target="_blank">Hironori Washizaki</a>,</nobr>
    
                    <nobr><a href="https://researchmap.jp/ubayashi" target="_blank">Naoyasu Ubayashi</a>,</nobr>
                and  
                    <nobr><a href="https://researchmap.jp/nobukazu" target="_blank">Nobukazu Yoshioka</a></nobr>
        </div>
    
        <div>
          <p class="periodical font-italic">
            
          </p>
        </div>
      
        <div class="col p-0">
          
            <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#VRTopic-abstract" role="button" aria-expanded="false" aria-controls="SLR-abstract">Abstract</a>
            <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/D1/CHASE_DECS.pdf" target="_blank">PDF</a>
            <a class="badge grey waves-effect font-weight-light mr-1" href="" target="_blank">Poster</a>
          
       
          </div>
      
        
        <div class="col mt-2 p-0">
          <div id="VRTopic-abstract" class="collapse">
            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
              With the rapid development of Virtual Reality (VR) technology, effectively understanding user 
              feedback has become a core task for improving user experience and optimizing system functionality. 
              However, extracting meaningful insights from VR user reviews remains challenging. Traditional topic 
              modeling methods often generate unannotated and ambiguous topics, requiring extensive manual annotation 
              and analysis. To address this issue, this study proposes an innovative approach that leverages 
              state-of-the-art Large Language Models (LLMs) to automatically identify and precisely summarize key topics
              from VR user reviews. Ultimately, this research aims to generate accurate topics from VR-related textual 
              inputs that genuinely reflect user concerns. By filling the gap in the application of LLMs to VR text 
              analysis, this study provides VR developers with precise user insights, aiding product optimization and 
              iterative improvement.
            </div>
          </div>
        </div>
        
      </div>
    </div>
    </div></li></ol>
  </div>
</div>




<!-- ==============2024============= -->

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
  <div class="col-sm-1 mt-2 p-0 pr-1">
    <h3 class="bibliography-year">2024</h3>
  </div>
  <div class="col-sm-11 p-0">
    <!-- News items for 2024 -->
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-1 p-0 abbr">
      <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://arxiv.org/abs/2409.06741" target="_blank">
        arXiv
      </a>
    </div>
    <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
  
      <div id="Analysis_of_STM" class="col p-0">
        <h5 class="title mb-0">Generative AI for Requirements Engineering: A Systematic Literature Review.</h5>
        <div class="author">
                  <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>,</nobr>
                
                    <nobr>Jati H. Husen,</nobr>
    
                    <nobr>Sien Reeve Peralta,</nobr>
    
                    <nobr>Bowen Jiang,</nobr>
    
                    <nobr><a href="https://researchmap.jp/nobukazu" target="_blank">Nobukazu Yoshioka</a>,</nobr>
    
                    <nobr><a href="https://researchmap.jp/ubayashi" target="_blank">Naoyasu Ubayashi</a>,</nobr>
                and  
                    <nobr><a href="http://www.washi.cs.waseda.ac.jp/washizaki/" target="_blank">Hironori Washizaki</a></nobr>
        </div>
    
        <div>
          <p class="periodical font-italic">
            
          </p>
        </div>
      
        <div class="col p-0">
          
            <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#SLR-abstract" role="button" aria-expanded="false" aria-controls="SLR-abstract">Abstract</a>
            <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/D1/SLR.pdf" target="_blank">PDF</a>
            <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/D1/SLR_Poster.pdf" target="_blank">Poster</a>
          
       
          </div>
      
        
        <div class="col mt-2 p-0">
          <div id="SLR-abstract" class="collapse">
            <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
              Context: Generative AI (GenAI) has emerged as a transformative tool in software engineering, with requirements engineering (RE) actively exploring its potential to revolutionize processes and outcomes. The integration of GenAI into RE presents both promising opportunities and significant challenges that necessitate systematic analysis and evaluation.

              Objective: This paper presents a comprehensive systematic literature review (SLR) analyzing state-of-the-art applications and innovative proposals leveraging GenAI in RE. It surveys studies focusing on the utilization of GenAI to enhance RE processes while identifying key challenges and opportunities in this rapidly evolving field. 

              Method: A rigorous SLR methodology was used to conduct an in-depth analysis of 27 carefully selected primary studies. The review examined research questions pertaining to the application of GenAI across various RE phases, the models and techniques used, and the challenges encountered in implementation and adoption.

              Results: The most salient findings include i: a predominant focus on the early stages of RE, particularly the elicitation and analysis of requirements, indicating potential for expansion into later phases; ii: the dominance of large language models, especially the GPT series, highlighting the need for diverse AI approaches; and iii: persistent challenges in domain-specific applications and the interpretability of AI-generated outputs, underscoring areas requiring further research and development.

              Conclusions: The results highlight the critical need for comprehensive evaluation frameworks, improved human–AI collaboration models, and thorough consideration of ethical implications in GenAI-assisted RE. Future research should prioritize extending GenAI applications across the entire RE lifecycle, enhancing domain-specific capabilities, and developing strategies for responsible AI integration in RE practices.
        
            </div>
          </div>
        </div>
        
      </div>
    </div>
    </div></li></ol>
  </div>
</div>



<!-- ==============2023============= -->


<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
  <div class="col-sm-1 mt-2 p-0 pr-1">
    <h3 class="bibliography-year">2023</h3>
  </div>
  <div class="col-sm-11 p-0">
    <ol class="bibliography">
      <li>
        <div class="row m-0 mt-3 p-0">
          <div class="col-sm-1 p-0 abbr">
            <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="http://www.apsipa.org/asc_web/apsipa2023/" target="_blank">
              APSIPA
            </a>
          </div>
          
          <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
            <div id="Analysis_of_STM" class="col p-0">
              <h5 class="title mb-0">Analysis of Spectro-Temporal Modulation Representation for Deep-Fake Speech Detection.</h5>
              <div class="author">
                <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>,</nobr>
                <nobr>Candy Olivia Mawalim,</nobr>
                <nobr>Kai Li,</nobr>
                <nobr>Lijun Wang,</nobr>
                and  
                <nobr><a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=293&l=1" target="_blank">Masashi Unoki</a>,</nobr>
              </div>

              <div>
                <p class="periodical font-italic">
                  The 15th Asia-Pasific Signal and Information Processing Association (APSIPA ASC 2023), Taipei, Taiwan, 31 October - 3 November 2023.
                </p>
              </div>
            
              <div class="col p-0">
                <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#deepfake-abstract" role="button" aria-expanded="false" aria-controls="deepfake-abstract">Abstract</a>
                <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfakes/Analysis_of_Spectro-Temporal_Modulation_Representation_for_Deep-Fake_Speech_Detection.pdf" target="_blank">PDF</a>
                <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfakes/deepfake_detection.pdf" target="_blank">Poster</a>
              </div>
            
              <div class="col mt-2 p-0">
                <div id="deepfake-abstract" class="collapse">
                  <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                    Deep-fake speech detection aims to develop effective techniques for identifying fake speech generated using advanced deep-learning methods. 
                    It can reduce the negative impact of malicious production or dissemination of fake speech in real-life scenarios. Although humans can relatively easy to distinguish between genuine and fake speech due to 
                    human auditory mechanisms, it is difficult for machines to distinguish them correctly. One major reason for this challenge is that machines struggle to effectively separate speech content from human vocal system information. 
                    Common features used in speech processing face difficulties in handling this issue, hindering the neural network from learning the discriminative differences between genuine and fake speech. To address this issue, 
                    we investigated spectro-temporal modulation representations in genuine and fake speech, which simulate the human auditory perception process. Next, the spectro-temporal modulation was fit to a light convolutional neural network 
                    bidirectional long short-term memory for classification. We conducted experiments on the benchmark datasets of the Automatic Speaker Verification and Spoofing Countermeasures Challenge 2019 (ASVspoof2019) and the Audio Deep synthesis 
                    Detection Challenge 2023 (ADD2023), achieving an equal-error rate of 8.33\% and 42.10\%, respectively. The results showed that spectro-temporal modulation representations could distinguish the genuine and deep-fake speech and have adequate 
                    performance in both datasets.
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>

      <li>
        <div class="row m-0 mt-3 p-0">
          <div class="col-sm-1 p-0 abbr">
            <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://www.ieice.org/hokuriku/" target="_blank">
              JHES
            </a>
          </div>
          
          <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
            <div id="deep-fake" class="col p-0">
              <h5 class="title mb-0">Study on Deep-Fake Speech Detection Based on Spectro-Temporal Modulation Representation.</h5>
              <div class="author">
                <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>, Candy Olivia Mawalim, Kai Li, and Masashi Unoki</nobr>
              </div>

              <div>
                <p class="periodical font-italic">
                  In Proc. Joint conference of Hokuriku chapters of Electrical and information Societies (JHES 2023).
                </p>
              </div>
            
              <div class="col p-0">
                <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfakes/jhes2023_Haowei_Final.pdf" target="_blank">PDF</a>
                <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfakes/JHES_Haowei_Slide.pdf" target="_blank">Slide</a>
              </div>
            
              <div class="col mt-2 p-0">
                <div id="agile_modeling-abstract" class="collapse">
                  <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                    [摘要内容保持不变]
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </li>
    </ol>
  </div>
</div>




  <!-- Footer -->
  <footer>
    &copy; Copyright 2024 Haowei Cheng.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
