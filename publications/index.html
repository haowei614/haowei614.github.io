<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Haowei Cheng | Publications</title>
  <meta name="description" content="This is the personal website of Haowei Cheng">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Haowei</span> Cheng</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About     
            </a>
          </li>

              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    Curriculum Vitae      
                  </a>
              </li>

              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                  </a>
              </li>

              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                      <span class="sr-only">(current)</span>
                  </a>
              </li>
 
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Publications</h1>
  <h6><nobr><em>*</em></nobr> denotes equal contribution and joint lead authorship.</h6>
<p></p>


<!-- ===========2024========== -->
<!-- <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
  <div class="col-sm-1 mt-2 p-0 pr-1">
    <h3 class="bibliography-year">2024</h3>
  </div>
  <div class="col-sm-11 p-0">
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
      <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://dblp.org/db/conf/apsec/index.html" target="_blank">
        APSEC
      </a>
</div>

<div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
  
  <div id="Analysis_of_STM" class="col p-0">
    <h5 class="title mb-0">SLR for GenAI and RE</h5>
    <div class="author">
              <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>,</nobr>
            

                <nobr>Candy Olivia Mawalim,</nobr>

                <nobr>Kai Li,</nobr>

                <nobr>Lijun Wang,</nobr>
            and  
                <nobr><a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=293&l=1" target="_blank">Masashi Unoki</a>,</nobr>
    </div>

    <div>
      <p class="periodical font-italic">
        
        The 31st Asia-Pacific Software Engineering Conference (APSEC 2024)
      </p>
    </div>
  
    <div class="col p-0">
      
        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#scaling_up_llm-abstract" role="button" aria-expanded="false" aria-controls="scaling_up_llm-abstract">Abstract</a>
        <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfakes/Analysis_of_Spectro-Temporal_Modulation_Representation_for_Deep-Fake_Speech_Detection.pdf" target="_blank">PDF</a>
        <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfake/deepfake_detection.pdf" target="_blank">Poster</a>

      </div>
 
  </div>
</div>
</div>

<div class="row m-0 mt-3 p-0">
<div class="col-sm-1 p-0 abbr">

      <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://aaai.org/conference/aaai/" target="_blank">
        AAAI
      </a>

</div>
<div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
  
</div>
</div>


</ol>
  </div>
</div> -->


<!-- ==============2024============= -->

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
  <div class="col-sm-1 mt-2 p-0 pr-1">
    <h3 class="bibliography-year">2024</h3>
  </div>
  <div class="col-sm-11 p-0">
    <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
      <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="http://www.apsipa.org/" target="_blank">
        arXiv
      </a>
</div>

<div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
  
  <div id="Analysis_of_STM" class="col p-0">
    <h5 class="title mb-0">Generative AI for Requirements Engineering: A Systematic Literature Review.</h5>
    <div class="author">
              <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>,</nobr>
            
                <nobr>Jati H. Husen,</nobr>

                <nobr>Sien Reeve Peralta,</nobr>

                <nobr>Bowen Jiang,</nobr>

                <nobr><a href="https://researchmap.jp/nobukazu" target="_blank">Nobukazu Yoshioka</a>,</nobr>

                <nobr><a href="https://researchmap.jp/ubayashi" target="_blank">Naoyasu Ubayashi</a>,</nobr>
            and  
                <nobr><a href="http://www.washi.cs.waseda.ac.jp/washizaki/" target="_blank">Hironori Washizaki</a>,</nobr>
    </div>

    <div>
      <p class="periodical font-italic">
        
          The 15th Asia-Pasific Signal and Information Processing Association (APSIPA ASC 2023), Taipei, Taiwan, 31 October - 3 November 2023.
        
      </p>
    </div>
  
    <div class="col p-0">
      
        <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#deepfake-abstract" role="button" aria-expanded="false" aria-controls="SLR-abstract">Abstract</a>
        <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/D1/SLR.pdf" target="_blank">PDF</a>
        <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/D1/SLR_Poster.pdf" target="_blank">Poster</a>
      
   
      </div>
  
    
    <div class="col mt-2 p-0">
      <div id="SLR-abstract" class="collapse">
        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
          Context: Generative AI (GenAI) has emerged as a transformative tool in software engineering, with requirements engineering (RE) actively exploring its potential to revolutionize processes and outcomes. The integration of GenAI into RE presents both promising opportunities and significant challenges that necessitate systematic analysis and evaluation.

          Objective: This paper presents a comprehensive systematic literature review (SLR) analyzing state-of-the-art applications and innovative proposals leveraging GenAI in RE. It surveys studies focusing on the utilization of GenAI to enhance RE processes while identifying key challenges and opportunities in this rapidly evolving field. 

          Method: A rigorous SLR methodology was used to conduct an in-depth analysis of 27 carefully selected primary studies. The review examined research questions pertaining to the application of GenAI across various RE phases, the models and techniques used, and the challenges encountered in implementation and adoption.

          Results: The most salient findings include i: a predominant focus on the early stages of RE, particularly the elicitation and analysis of requirements, indicating potential for expansion into later phases; ii: the dominance of large language models, especially the GPT series, highlighting the need for diverse AI approaches; and iii: persistent challenges in domain-specific applications and the interpretability of AI-generated outputs, underscoring areas requiring further research and development.

          Conclusions: The results highlight the critical need for comprehensive evaluation frameworks, improved human–AI collaboration models, and thorough consideration of ethical implications in GenAI-assisted RE. Future research should prioritize extending GenAI applications across the entire RE lifecycle, enhancing domain-specific capabilities, and developing strategies for responsible AI integration in RE practices.
        </div>
        
      </div>
    </div>
    
  </div>
</div>
</div>




<!-- ==============2023============= -->
<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2023</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-1 p-0 abbr">
        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="http://www.apsipa.org/" target="_blank">
          APSIPA
        </a>
  </div>
  
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Analysis_of_STM" class="col p-0">
      <h5 class="title mb-0">Analysis of Spectro-Temporal Modulation Representation for Deep-Fake Speech Detection.</h5>
      <div class="author">
                <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>,</nobr>
              

                  <nobr>Candy Olivia Mawalim,</nobr>
 
                  <nobr>Kai Li,</nobr>

                  <nobr>Lijun Wang,</nobr>
              and  
                  <nobr><a href="https://fp.jaist.ac.jp/public/Default2.aspx?id=293&l=1" target="_blank">Masashi Unoki</a>,</nobr>
      </div>

      <div>
        <p class="periodical font-italic">
          
            The 15th Asia-Pasific Signal and Information Processing Association (APSIPA ASC 2023), Taipei, Taiwan, 31 October - 3 November 2023.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#deepfake-abstract" role="button" aria-expanded="false" aria-controls="deepfake-abstract">Abstract</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfakes/Analysis_of_Spectro-Temporal_Modulation_Representation_for_Deep-Fake_Speech_Detection.pdf" target="_blank">PDF</a>
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfakes/deepfake_detection.pdf" target="_blank">Poster</a>
        
     
        </div>
    
      
      <div class="col mt-2 p-0">
        <div id="deepfake-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Deep-fake speech detection aims to develop effective techniques for identifying fake speech generated using advanced deep-learning methods. 
            It can reduce the negative impact of malicious production or dissemination of fake speech in real-life scenarios. Although humans can relatively easy to distinguish between genuine and fake speech due to 
            human auditory mechanisms, it is difficult for machines to distinguish them correctly. One major reason for this challenge is that machines struggle to effectively separate speech content from human vocal system information. 
            Common features used in speech processing face difficulties in handling this issue, hindering the neural network from learning the discriminative differences between genuine and fake speech. To address this issue, 
            we investigated spectro-temporal modulation representations in genuine and fake speech, which simulate the human auditory perception process. Next, the spectro-temporal modulation was fit to a light convolutional neural network 
            bidirectional long short-term memory for classification. We conducted experiments on the benchmark datasets of the Automatic Speaker Verification and Spoofing Countermeasures Challenge 2019 (ASVspoof2019) and the Audio Deep synthesis 
            Detection Challenge 2023 (ADD2023), achieving an equal-error rate of 8.33\% and 42.10\%, respectively. The results showed that spectro-temporal modulation representations could distinguish the genuine and deep-fake speech and have adequate 
            performance in both datasets.
          </div>
          
        </div>
      </div>
      
    </div>
  </div>
</div>

<div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">

        <a class="badge font-weight-bold light-green darken-1 align-middle" style="width: 65px;" href="https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings" target="_blank">
          JHES
        </a>

  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="deep-fake" class="col p-0">
      <h5 class="title mb-0">Study on Deep-Fake Speech Detection
        Based on Spectro-Temporal Modulation Representation.</h5>
      <div class="author">
 
                <nobr><em>Haowei Cheng<nobr><em>*</em></nobr></em>, Candy Olivia Mawalim, Kai Li, and Masashi Unoki</nobr>
        
      </div>

      <div>
        <p class="periodical font-italic">
          
          In Proc. Joint conference of Hokuriku chapters of
          Electrical and information Societies (JHES 2023).

          
        </p>
      </div>
    
      <div class="col p-0">

          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfakes/jhes2023_Haowei_Final.pdf" target="_blank">PDF</a>
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/deepfakes/JHES_Haowei_Slide.pdf" target="_blank">Slide</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="agile_modeling-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            The application of computer vision methods to nuanced, subjective concepts is growing. While crowdsourcing has served the vision community well for most objective tasks (such as labeling a “zebra”), it now falters on tasks where there is substantial subjectivity in the concept (such as identifying “gourmet tuna”). However, empowering any user to develop a classifier for their concept is technically difficult: users are neither machine learning experts nor have the patience to label thousands of examples. In reaction, we introduce the problem of Agile Modeling: the process of turning any subjective visual concept into a computer vision model through real-time user-in-the-loop interactions. We instantiate an Agile Modeling prototype for image classification and show through a user study (N=14) that users can create classifiers with minimal effort in under 30 minutes. We compare this user driven process with the traditional crowdsourcing paradigm and find that the crowd’s notion often differs from that of the user’s, especially as the concepts become more subjective. Finally, we scale our experiments with simulations of users training classifiers for ImageNet21k categories to further demonstrate the efficacy of the approach.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>


</ol>
    </div>
  </div>
  

  
 <!-- bottom line and year -->
<!-- <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2023</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
     -->


  <!-- Footer -->
  <footer>
    &copy; Copyright 2024 Haowei Cheng.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
